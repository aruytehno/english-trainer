#!/usr/bin/env python3
"""
–ü–∞—Ä—Å–µ—Ä Oxford 3000 / 5000 Key Words (PDF) ‚Üí words.json

–¢—Ä–µ–±–æ–≤–∞–Ω–∏—è:
    pip install pdfplumber

–ó–∞–ø—É—Å–∫:
    python parse_oxford.py
"""

import pdfplumber
import json
import re
from pathlib import Path
import sys
from collections import defaultdict

# -------------------- –ù–ê–°–¢–†–û–ô–ö–ò --------------------

PDF_FILES = [
    "Oxford-3000-Key-Words.pdf",
    "Oxford-5000-Key-Words.pdf",
]

OUTPUT_FILE = "words.json"

# –£—Ä–æ–≤–Ω–∏ –≤ –ø–æ—Ä—è–¥–∫–µ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ –¥–ª—è —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∏
LEVEL_ORDER = ["A1", "A2", "B1", "B2", "C1"]
LEVELS_SET = set(LEVEL_ORDER)

# –†–µ–≥—É–ª—è—Ä–∫–∞ –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —á–∏—Å—Ç–æ–≥–æ —Å–ª–æ–≤–∞ (–∏–≥–Ω–æ—Ä–∏—Ä—É–µ–º —á–∞—Å—Ç–∏ —Ä–µ—á–∏, —Å–∫–æ–±–∫–∏)
WORD_RE = re.compile(r"^([a-zA-Z][a-zA-Z\-]*)")


# --------------------------------------------------


def parse_oxford_3000_pdf(path: Path, start_id: int):
    """
    –ü–∞—Ä—Å–∏—Ç Oxford 3000 PDF (A1-B2 —É—Ä–æ–≤–Ω–∏)
    """
    words = []
    current_level = None
    word_id = start_id

    with pdfplumber.open(path) as pdf:
        for page in pdf.pages:
            text = page.extract_text()
            if not text:
                continue

            for line in text.splitlines():
                line = line.strip()

                # –í Oxford 3000 —É—Ä–æ–≤–Ω–∏ –æ–±–æ–∑–Ω–∞—á–µ–Ω—ã –∫–∞–∫ A1, A2, B1, B2
                if line in ["A1", "A2", "B1", "B2"]:
                    current_level = line
                    continue

                # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –∑–∞–≥–æ–ª–æ–≤–∫–∏ –∏ –ø—É—Å—Ç—ã–µ —Å—Ç—Ä–æ–∫–∏
                if (not line or
                        "Oxford" in line or
                        "¬©" in line or
                        line.isdigit() or
                        "/" in line):
                    continue

                if not current_level:
                    continue

                # –ò–∑–≤–ª–µ–∫–∞–µ–º —Å–ª–æ–≤–æ (–∏–≥–Ω–æ—Ä–∏—Ä—É–µ–º —á–∞—Å—Ç–∏ —Ä–µ—á–∏)
                # –ü—Ä–∏–º–µ—Ä—ã —Å—Ç—Ä–æ–∫: "absorb v", "abstract adj.", "match (contest/correspond) n., v."
                clean_line = line.split()[0]  # –ë–µ—Ä–µ–º –ø–µ—Ä–≤–æ–µ —Å–ª–æ–≤–æ

                # –£–±–∏—Ä–∞–µ–º —Å–∫–æ–±–∫–∏ –∏ –∏—Ö —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ
                if '(' in clean_line:
                    clean_line = clean_line.split('(')[0].strip()

                # –£–±–∏—Ä–∞–µ–º —Ü–∏—Ñ—Ä—ã –≤ –∫–æ–Ω—Ü–µ (–Ω–∞–ø—Ä–∏–º–µ—Ä, "rose2")
                clean_line = re.sub(r'\d+$', '', clean_line)

                if not clean_line or not clean_line.isalpha():
                    continue

                word = clean_line.lower()

                words.append({
                    "id": word_id,
                    "en": word,
                    "ru": "",
                    "level": current_level
                })
                word_id += 1

    return words, word_id


def parse_oxford_5000_pdf(path: Path, start_id: int):
    """
    –ü–∞—Ä—Å–∏—Ç Oxford 5000 PDF (B2-C1 —É—Ä–æ–≤–Ω–∏)
    """
    words = []
    current_level = None
    word_id = start_id
    in_b2_c1_section = False

    with pdfplumber.open(path) as pdf:
        for page in pdf.pages:
            text = page.extract_text()
            if not text:
                continue

            for line in text.splitlines():
                line = line.strip()

                # –í Oxford 5000 –µ—Å—Ç—å —Å–µ–∫—Ü–∏–∏ B2 –∏ C1
                if line == "B2" or line == "C1":
                    current_level = line
                    in_b2_c1_section = True
                    continue

                # –ó–∞–≥–æ–ª–æ–≤–æ–∫ –¥–æ–∫—É–º–µ–Ω—Ç–∞ - –≤—ã—Ö–æ–¥–∏–º –∏–∑ —Å–µ–∫—Ü–∏–∏
                if "The Oxford 5000‚Ñ¢ by CEFR level" in line:
                    in_b2_c1_section = False
                    continue

                # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Å—Ç—Ä–æ–∫–∏ –≤–Ω–µ —Å–µ–∫—Ü–∏–π B2/C1
                if not in_b2_c1_section or not current_level:
                    continue

                # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –∑–∞–≥–æ–ª–æ–≤–∫–∏ –∏ –ø—É—Å—Ç—ã–µ —Å—Ç—Ä–æ–∫–∏
                if (not line or
                        "Oxford" in line or
                        "¬©" in line or
                        "/" in line):
                    continue

                # –ò–∑–≤–ª–µ–∫–∞–µ–º —Å–ª–æ–≤–æ (–∞–Ω–∞–ª–æ–≥–∏—á–Ω–æ 3000)
                clean_line = line.split()[0]

                if '(' in clean_line:
                    clean_line = clean_line.split('(')[0].strip()

                clean_line = re.sub(r'\d+$', '', clean_line)

                if not clean_line or not clean_line.isalpha():
                    continue

                word = clean_line.lower()

                words.append({
                    "id": word_id,
                    "en": word,
                    "ru": "",
                    "level": current_level
                })
                word_id += 1

    return words, word_id


def deduplicate(words):
    """
    –£–±–∏—Ä–∞–µ—Ç –¥—É–±–ª–∏–∫–∞—Ç—ã –ø–æ –ø–æ–ª—é 'en' (–æ–¥–Ω–æ —Å–ª–æ–≤–æ –º–æ–∂–µ—Ç –±—ã—Ç—å —Ç–æ–ª—å–∫–æ –æ–¥–∏–Ω —Ä–∞–∑)
    –°–æ—Ö—Ä–∞–Ω—è–µ—Ç –±–æ–ª–µ–µ –Ω–∏–∑–∫–∏–π —É—Ä–æ–≤–µ–Ω—å –¥–ª—è –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ (A1 –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω–µ–µ C1)
    """
    level_priority = {level: i for i, level in enumerate(LEVEL_ORDER)}

    unique_words = {}

    for w in words:
        word = w["en"]
        level = w["level"]

        if word not in unique_words:
            unique_words[word] = w
        else:
            # –ï—Å–ª–∏ —Å–ª–æ–≤–æ —É–∂–µ –µ—Å—Ç—å, –≤—ã–±–∏—Ä–∞–µ–º –±–æ–ª–µ–µ –Ω–∏–∑–∫–∏–π —É—Ä–æ–≤–µ–Ω—å
            existing_level = unique_words[word]["level"]
            if level_priority[level] < level_priority[existing_level]:
                unique_words[word] = w

    return list(unique_words.values())


def sort_by_level_and_alphabet(words):
    """
    –°–æ—Ä—Ç–∏—Ä—É–µ—Ç —Å–ª–æ–≤–∞: —Å–Ω–∞—á–∞–ª–∞ –ø–æ —É—Ä–æ–≤–Ω—é (A1‚ÜíC1), –ø–æ—Ç–æ–º –ø–æ –∞–ª—Ñ–∞–≤–∏—Ç—É
    """
    level_order = {level: i for i, level in enumerate(LEVEL_ORDER)}

    return sorted(words, key=lambda x: (level_order[x["level"]], x["en"]))


def renumber(words):
    """
    –ü–µ—Ä–µ–Ω—É–º–µ—Ä–æ–≤—ã–≤–∞–µ—Ç id –ø–æ–¥—Ä—è–¥
    """
    for i, w in enumerate(words, start=1):
        w["id"] = i
    return words


def print_statistics(words):
    """
    –í—ã–≤–æ–¥–∏—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ —É—Ä–æ–≤–Ω—è–º
    """
    stats = defaultdict(int)
    for w in words:
        stats[w["level"]] += 1

    print("\nüìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —É—Ä–æ–≤–Ω—è–º:")
    print("-" * 30)
    for level in LEVEL_ORDER:
        if level in stats:
            print(f"  {level}: {stats[level]:4d} —Å–ª–æ–≤")
    print("-" * 30)
    print(f"  –í—Å–µ–≥–æ: {len(words):4d} —Å–ª–æ–≤")

    # –ü—Ä–∏–º–µ—Ä—ã —Å–ª–æ–≤ –∫–∞–∂–¥–æ–≥–æ —É—Ä–æ–≤–Ω—è
    print("\nüî§ –ü—Ä–∏–º–µ—Ä—ã —Å–ª–æ–≤ –ø–æ —É—Ä–æ–≤–Ω—è–º:")
    for level in LEVEL_ORDER:
        level_words = [w["en"] for w in words if w["level"] == level][:5]
        if level_words:
            print(f"  {level}: {', '.join(level_words)}")


def main():
    all_words = []
    current_id = 1

    for pdf_name in PDF_FILES:
        pdf_path = Path(pdf_name)

        if not pdf_path.exists():
            print(f"‚ùå –§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {pdf_name}")
            sys.exit(1)

        print(f"üìÑ –û–±—Ä–∞–±–∞—Ç—ã–≤–∞—é {pdf_name} ...")

        if "3000" in pdf_name:
            parsed, current_id = parse_oxford_3000_pdf(pdf_path, current_id)
        elif "5000" in pdf_name:
            parsed, current_id = parse_oxford_5000_pdf(pdf_path, current_id)
        else:
            print(f"‚ö†Ô∏è  –ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —Ñ–∞–π–ª: {pdf_name}, –ø—Ä–æ–ø—É—Å–∫–∞—é")
            continue

        all_words.extend(parsed)
        print(f"  –ò–∑–≤–ª–µ—á–µ–Ω–æ: {len(parsed)} —Å–ª–æ–≤")

    print(f"\nüîÅ –£–¥–∞–ª—è—é –¥—É–±–ª–∏–∫–∞—Ç—ã...")
    all_words = deduplicate(all_words)
    print(f"  –ü–æ—Å–ª–µ —É–¥–∞–ª–µ–Ω–∏—è –¥—É–±–ª–∏–∫–∞—Ç–æ–≤: {len(all_words)} —Å–ª–æ–≤")

    print(f"\nüîÄ –°–æ—Ä—Ç–∏—Ä—É—é –ø–æ —É—Ä–æ–≤–Ω—è–º –∏ –∞–ª—Ñ–∞–≤–∏—Ç—É...")
    all_words = sort_by_level_and_alphabet(all_words)

    print(f"\nüî¢ –ü–µ—Ä–µ–Ω—É–º–µ—Ä–æ–≤—ã–≤–∞—é...")
    all_words = renumber(all_words)

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
    with open(OUTPUT_FILE, "w", encoding="utf-8") as f:
        json.dump(all_words, f, ensure_ascii=False, indent=2)

    print(f"\n‚úÖ –ì–æ—Ç–æ–≤–æ!")
    print(f"üíæ –§–∞–π–ª: {OUTPUT_FILE}")

    # –í—ã–≤–æ–¥–∏–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
    print_statistics(all_words)

    # –ü—Ä–∏–º–µ—Ä –ø–µ—Ä–≤—ã—Ö 20 —Å–ª–æ–≤
    print(f"\nüìù –ü–µ—Ä–≤—ã–µ 20 —Å–ª–æ–≤:")
    print("-" * 40)
    for i, word in enumerate(all_words[:20], 1):
        print(f"{word['id']:4d}. {word['en']:20s} [{word['level']}]")


if __name__ == "__main__":
    main()