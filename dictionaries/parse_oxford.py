#!/usr/bin/env python3
"""
–ü–∞—Ä—Å–µ—Ä Oxford 3000 / 5000 –∏–∑ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö —Ñ–∞–π–ª–æ–≤ ‚Üí words.json
–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ç–æ–ª—å–∫–æ —á–∏—Å—Ç—ã–µ —Å–ª–æ–≤–∞ –±–µ–∑ —á–∞—Å—Ç–µ–π —Ä–µ—á–∏.

–ò—Å–ø–æ–ª—å–∑—É–µ—Ç –≥–æ—Ç–æ–≤—ã–µ —Ç–µ–∫—Å—Ç–æ–≤—ã–µ —Ñ–∞–π–ª—ã:
    dictionaries/A1 (3000).txt
    dictionaries/A2 (3000).txt
    dictionaries/B1 (3000).txt
    dictionaries/B2 (3000).txt
    dictionaries/B2 (5000).txt
    dictionaries/C1 (5000).txt

–ó–∞–ø—É—Å–∫:
    python parse_oxford.py
"""

import json
import re
from pathlib import Path
import sys
from collections import defaultdict

# -------------------- –ù–ê–°–¢–†–û–ô–ö–ò --------------------

INPUT_FILES = {
    "A1": "A1 (3000).txt",
    "A2": "A2 (3000).txt",
    "B1": "B1 (3000).txt",
    "B2": "B2 (3000).txt",
    "B2_5000": "B2 (5000).txt",  # Oxford 5000 B2 —Å–ª–æ–≤–∞
    "C1": "C1 (5000).txt",
}

OUTPUT_FILE = "words.json"

# –ü–æ—Ä—è–¥–æ–∫ —É—Ä–æ–≤–Ω–µ–π –¥–ª—è —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∏
LEVEL_ORDER = ["A1", "A2", "B1", "B2", "C1"]

# –†–µ–≥—É–ª—è—Ä–∫–∞ –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —á–∏—Å—Ç–æ–≥–æ —Å–ª–æ–≤–∞
# –ü—Ä–∏–º–µ—Ä—ã:
#   "a, an indefinite article" ‚Üí "a"
#   "about prep., adv." ‚Üí "about"
#   "match (contest/correspond) n., v." ‚Üí "match"
CLEAN_WORD_RE = re.compile(r'^([a-zA-Z]+)')


# --------------------------------------------------

def extract_clean_word(line):
    """
    –ò–∑–≤–ª–µ–∫–∞–µ—Ç —á–∏—Å—Ç–æ–µ —Å–ª–æ–≤–æ –∏–∑ —Å—Ç—Ä–æ–∫–∏
    """
    # –£–±–∏—Ä–∞–µ–º –≤—Å—ë –ø–æ—Å–ª–µ –ø–µ—Ä–≤–æ–≥–æ –Ω–µ-–±—É–∫–≤–µ–Ω–Ω–æ–≥–æ —Å–∏–º–≤–æ–ª–∞
    match = CLEAN_WORD_RE.match(line)
    if match:
        return match.group(1).lower()

    # –ï—Å–ª–∏ —Ä–µ–≥—É–ª—è—Ä–∫–∞ –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª–∞, –ø—Ä–æ–±—É–µ–º –¥—Ä—É–≥–æ–π —Å–ø–æ—Å–æ–±
    # –†–∞–∑–¥–µ–ª—è–µ–º –ø–æ –ø—Ä–æ–±–µ–ª–∞–º –∏ –±–µ—Ä–µ–º –ø–µ—Ä–≤–æ–µ —Å–ª–æ–≤–æ
    first_part = line.split()[0] if ' ' in line else line

    # –£–±–∏—Ä–∞–µ–º –∑–∞–ø—è—Ç—ã–µ, —Ç–æ—á–∫–∏, —Å–∫–æ–±–∫–∏
    clean = re.sub(r'[,\\.()]', '', first_part)
    return clean.lower()


def parse_level_file(filepath: Path, level: str):
    """
    –ü–∞—Ä—Å–∏—Ç —Ñ–∞–π–ª –æ–¥–Ω–æ–≥–æ —É—Ä–æ–≤–Ω—è –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ —á–∏—Å—Ç—ã—Ö —Å–ª–æ–≤
    """
    words = []
    seen_words = set()  # –î–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ –≤–Ω—É—Ç—Ä–∏ –æ–¥–Ω–æ–≥–æ —Ñ–∞–π–ª–∞

    if not filepath.exists():
        print(f"‚ùå –§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {filepath}")
        return words

    with open(filepath, 'r', encoding='utf-8') as f:
        for line in f:
            line = line.strip()

            # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –ø—É—Å—Ç—ã–µ —Å—Ç—Ä–æ–∫–∏ –∏ –∑–∞–≥–æ–ª–æ–≤–∫–∏
            if not line or "Oxford University Press" in line or line.startswith("¬©"):
                continue

            # –ò–∑–≤–ª–µ–∫–∞–µ–º —á–∏—Å—Ç–æ–µ —Å–ª–æ–≤–æ
            clean_word = extract_clean_word(line)

            # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –µ—Å–ª–∏ –Ω–µ –∏–∑–≤–ª–µ–∫–ª–∏ —Å–ª–æ–≤–æ –∏–ª–∏ —ç—Ç–æ —Ü–∏—Ñ—Ä–∞
            if not clean_word or clean_word.isdigit():
                continue

            # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã –≤–Ω—É—Ç—Ä–∏ —Ñ–∞–π–ª–∞
            if clean_word in seen_words:
                continue

            seen_words.add(clean_word)
            words.append({
                "en": clean_word,
                "ru": "",
                "level": level
            })

    return words


def deduplicate_preserve_levels(words):
    """
    –£–±–∏—Ä–∞–µ—Ç –¥—É–±–ª–∏–∫–∞—Ç—ã —Å–ª–æ–≤.
    –ï—Å–ª–∏ —Å–ª–æ–≤–æ –µ—Å—Ç—å –≤ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —É—Ä–æ–≤–Ω—è—Ö, —Å–æ—Ö—Ä–∞–Ω—è–µ–º —Å–∞–º—ã–π –Ω–∏–∑–∫–∏–π —É—Ä–æ–≤–µ–Ω—å.
    """
    # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç —É—Ä–æ–≤–Ω–µ–π (A1 —Å–∞–º—ã–π –≤—ã—Å–æ–∫–∏–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç)
    level_priority = {level: i for i, level in enumerate(LEVEL_ORDER)}

    unique_words = {}

    for w in words:
        word = w["en"]
        level = w["level"]

        if word not in unique_words:
            unique_words[word] = w
        else:
            # –ï—Å–ª–∏ —Å–ª–æ–≤–æ —É–∂–µ –µ—Å—Ç—å, –ø—Ä–æ–≤–µ—Ä—è–µ–º —É—Ä–æ–≤–µ–Ω—å
            existing_level = unique_words[word]["level"]
            if level_priority[level] < level_priority[existing_level]:
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å–ª–æ–≤–æ —Å –±–æ–ª–µ–µ –Ω–∏–∑–∫–∏–º —É—Ä–æ–≤–Ω–µ–º (A1 –ª—É—á—à–µ —á–µ–º B2)
                unique_words[word] = w

    return list(unique_words.values())


def sort_words_by_level(words):
    """
    –°–æ—Ä—Ç–∏—Ä—É–µ—Ç —Å–ª–æ–≤–∞: —Å–Ω–∞—á–∞–ª–∞ –ø–æ —É—Ä–æ–≤–Ω—é (A1‚ÜíC1), –ø–æ—Ç–æ–º –ø–æ –∞–ª—Ñ–∞–≤–∏—Ç—É
    """
    level_order = {level: i for i, level in enumerate(LEVEL_ORDER)}

    return sorted(words, key=lambda w: (level_order[w["level"]], w["en"]))


def print_statistics(words):
    """
    –í—ã–≤–æ–¥–∏—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ —É—Ä–æ–≤–Ω—è–º
    """
    stats = defaultdict(int)
    for w in words:
        stats[w["level"]] += 1

    print("\nüìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —É—Ä–æ–≤–Ω—è–º:")
    print("-" * 30)
    total = 0
    for level in LEVEL_ORDER:
        count = stats.get(level, 0)
        total += count
        print(f"  {level}: {count:4d} —Å–ª–æ–≤")
    print("-" * 30)
    print(f"  –í—Å–µ–≥–æ: {total:4d} —Å–ª–æ–≤")

    # –ü—Ä–∏–º–µ—Ä—ã —Å–ª–æ–≤ –∫–∞–∂–¥–æ–≥–æ —É—Ä–æ–≤–Ω—è
    print("\nüî§ –ü—Ä–∏–º–µ—Ä—ã —Å–ª–æ–≤ –ø–æ —É—Ä–æ–≤–Ω—è–º (–ø–µ—Ä–≤—ã–µ 5):")
    for level in LEVEL_ORDER:
        level_words = [w["en"] for w in words if w["level"] == level][:5]
        if level_words:
            print(f"  {level}: {', '.join(level_words)}")


def main():
    print("üìö –ü–∞—Ä—Å–∏–Ω–≥ Oxford —Å–ª–æ–≤–∞—Ä–µ–π –∏–∑ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö —Ñ–∞–π–ª–æ–≤")
    print("=" * 50)

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –º—ã –≤ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–π –ø–∞–ø–∫–µ
    if not Path("A1 (3000).txt").exists():
        print("‚ùå –§–∞–π–ª—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –≤ —Ç–µ–∫—É—â–µ–π –ø–∞–ø–∫–µ.")
        print("   –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ —Å–∫—Ä–∏–ø—Ç –∑–∞–ø—É—Å–∫–∞–µ—Ç—Å—è –∏–∑ –ø–∞–ø–∫–∏ dictionaries/")
        sys.exit(1)

    all_words = []

    # –ü–∞—Ä—Å–∏–º –≤—Å–µ —Ñ–∞–π–ª—ã
    for level_name, filename in INPUT_FILES.items():
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —É—Ä–æ–≤–µ–Ω—å CEFR –∏–∑ –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞
        if level_name in ["A1", "A2", "B1", "B2", "C1"]:
            level = level_name
        elif level_name == "B2_5000":
            level = "B2"  # Oxford 5000 B2 —Å–ª–æ–≤–∞
        else:
            level = level_name

        print(f"üìÑ –ß–∏—Ç–∞—é {filename} ...")
        words = parse_level_file(Path(filename), level)
        all_words.extend(words)
        print(f"  –ò–∑–≤–ª–µ—á–µ–Ω–æ: {len(words)} —á–∏—Å—Ç—ã—Ö —Å–ª–æ–≤")

    print(f"\nüîÅ –£–¥–∞–ª—è—é –¥—É–±–ª–∏–∫–∞—Ç—ã –º–µ–∂–¥—É —É—Ä–æ–≤–Ω—è–º–∏...")
    before_count = len(all_words)
    all_words = deduplicate_preserve_levels(all_words)
    after_count = len(all_words)
    print(f"  –ë—ã–ª–æ: {before_count}, —Å—Ç–∞–ª–æ: {after_count} (—É–¥–∞–ª–µ–Ω–æ: {before_count - after_count})")

    print(f"\nüîÄ –°–æ—Ä—Ç–∏—Ä—É—é –ø–æ —É—Ä–æ–≤–Ω—è–º –∏ –∞–ª—Ñ–∞–≤–∏—Ç—É...")
    all_words = sort_words_by_level(all_words)

    # –î–æ–±–∞–≤–ª—è–µ–º ID
    for i, word in enumerate(all_words, 1):
        word["id"] = i

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
    with open(OUTPUT_FILE, "w", encoding="utf-8") as f:
        json.dump(all_words, f, ensure_ascii=False, indent=2)

    print(f"\n‚úÖ –ì–æ—Ç–æ–≤–æ!")
    print(f"üíæ –§–∞–π–ª: {OUTPUT_FILE}")

    # –í—ã–≤–æ–¥–∏–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
    print_statistics(all_words)

    # –ü—Ä–∏–º–µ—Ä –ø–µ—Ä–≤—ã—Ö 20 —Å–ª–æ–≤
    print(f"\nüìù –ü–µ—Ä–≤—ã–µ 20 —Å–ª–æ–≤:")
    print("-" * 40)
    for i, word in enumerate(all_words[:20], 1):
        print(f"{word['id']:4d}. {word['en']:15s} [{word['level']}]")

    # –ü—Ä–∏–º–µ—Ä –ø–æ—Å–ª–µ–¥–Ω–∏—Ö 5 —Å–ª–æ–≤
    print(f"\nüìù –ü–æ—Å–ª–µ–¥–Ω–∏–µ 5 —Å–ª–æ–≤:")
    print("-" * 40)
    for word in all_words[-5:]:
        print(f"{word['id']:4d}. {word['en']:15s} [{word['level']}]")


if __name__ == "__main__":
    main()