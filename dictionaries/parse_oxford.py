#!/usr/bin/env python3
"""
–ü–∞—Ä—Å–µ—Ä Oxford 3000 / 5000 –∏–∑ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö —Ñ–∞–π–ª–æ–≤ ‚Üí words.json

–ò—Å–ø–æ–ª—å–∑—É–µ—Ç –≥–æ—Ç–æ–≤—ã–µ —Ç–µ–∫—Å—Ç–æ–≤—ã–µ —Ñ–∞–π–ª—ã:
    dictionaries/A1 (3000).txt
    dictionaries/A2 (3000).txt
    dictionaries/B1 (3000).txt
    dictionaries/B2 (3000).txt
    dictionaries/B2 (5000).txt
    dictionaries/C1 (5000).txt

–ó–∞–ø—É—Å–∫:
    python parse_oxford.py
"""

import json
import re
from pathlib import Path
import sys
from collections import defaultdict

# -------------------- –ù–ê–°–¢–†–û–ô–ö–ò --------------------

INPUT_FILES = {
    "A1": "A1 (3000).txt",
    "A2": "A2 (3000).txt",
    "B1": "B1 (3000).txt",
    "B2": "B2 (3000).txt",
    "B2_5000": "B2 (5000).txt",  # Oxford 5000 B2 —Å–ª–æ–≤–∞
    "C1": "C1 (5000).txt",
}

OUTPUT_FILE = "words.json"

# –ü–æ—Ä—è–¥–æ–∫ —É—Ä–æ–≤–Ω–µ–π –¥–ª—è —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∏
LEVEL_ORDER = ["A1", "A2", "B1", "B2", "C1"]


# --------------------------------------------------

def parse_level_file(filepath: Path, level: str):
    """
    –ü–∞—Ä—Å–∏—Ç —Ñ–∞–π–ª –æ–¥–Ω–æ–≥–æ —É—Ä–æ–≤–Ω—è –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ —Å–ª–æ–≤

    –§–æ—Ä–º–∞—Ç —Å—Ç—Ä–æ–∫–∏: "word part_of_speech" –∏–ª–∏ "word part_of_speech, part_of_speech"
    –ü—Ä–∏–º–µ—Ä: "even adv.", "match (contest/correspond) n., v."
    """
    words = []

    if not filepath.exists():
        print(f"‚ùå –§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {filepath}")
        return words

    with open(filepath, 'r', encoding='utf-8') as f:
        content = f.read()

        # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ —Å—Ç—Ä–æ–∫–∏ –∏ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞–∂–¥—É—é
        for line in content.splitlines():
            line = line.strip()

            # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –ø—É—Å—Ç—ã–µ —Å—Ç—Ä–æ–∫–∏ –∏ –∑–∞–≥–æ–ª–æ–≤–∫–∏
            if not line or "Oxford University Press" in line:
                continue

            # –†–∞–∑–¥–µ–ª—è–µ–º –ø–µ—Ä–≤–æ–µ —Å–ª–æ–≤–æ –∏ –æ—Å—Ç–∞–ª—å–Ω–æ–µ
            # –ü—Ä–∏–º–µ—Ä: "even adv." -> word="even adv.", level="A1"
            # –ú—ã —Å–æ—Ö—Ä–∞–Ω—è–µ–º –í–°–Æ —Å—Ç—Ä–æ–∫—É –∫–∞–∫ –µ—Å—Ç—å
            if line:
                # –£–±–∏—Ä–∞–µ–º –Ω–æ–º–µ—Ä —Å—Ç—Ä–∞–Ω–∏—Ü—ã –µ—Å–ª–∏ –µ—Å—Ç—å
                if line.startswith("¬©"):
                    continue

                # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å—Ç—Ä–æ–∫—É –ø–æ–ª–Ω–æ—Å—Ç—å—é –∫–∞–∫ –∞–Ω–≥–ª–∏–π—Å–∫–æ–µ —Å–ª–æ–≤–æ
                english_word = line.strip()

                words.append({
                    "en": english_word,
                    "ru": "",
                    "level": level
                })

    return words


def deduplicate_preserve_levels(words):
    """
    –£–±–∏—Ä–∞–µ—Ç –¥—É–±–ª–∏–∫–∞—Ç—ã –ø–æ —á–∏—Å—Ç–æ–º—É —Å–ª–æ–≤—É (–±–µ–∑ —á–∞—Å—Ç–∏ —Ä–µ—á–∏)
    –ï—Å–ª–∏ —Å–ª–æ–≤–æ –µ—Å—Ç—å –≤ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —É—Ä–æ–≤–Ω—è—Ö, —Å–æ—Ö—Ä–∞–Ω—è–µ–º —Å–∞–º—ã–π –Ω–∏–∑–∫–∏–π —É—Ä–æ–≤–µ–Ω—å
    """

    # –ò–∑–≤–ª–µ–∫–∞–µ–º —á–∏—Å—Ç–æ–µ —Å–ª–æ–≤–æ –∏–∑ —Å—Ç—Ä–æ–∫–∏ (–ø–µ—Ä–≤–æ–µ —Å–ª–æ–≤–æ –¥–æ –ø—Ä–æ–±–µ–ª–∞)
    def extract_base_word(entry):
        en = entry["en"]
        # –ë–µ—Ä–µ–º –ø–µ—Ä–≤–æ–µ —Å–ª–æ–≤–æ –¥–æ –ø—Ä–æ–±–µ–ª–∞, –∏–≥–Ω–æ—Ä–∏—Ä—É—è —Å–∫–æ–±–∫–∏
        match = re.match(r'^([a-zA-Z\-]+)', en)
        if match:
            return match.group(1).lower()
        return en.split()[0].lower() if ' ' in en else en.lower()

    # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç —É—Ä–æ–≤–Ω–µ–π (A1 —Å–∞–º—ã–π –≤—ã—Å–æ–∫–∏–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç)
    level_priority = {level: i for i, level in enumerate(LEVEL_ORDER)}

    unique_words = {}

    for w in words:
        base_word = extract_base_word(w)
        level = w["level"]

        if base_word not in unique_words:
            unique_words[base_word] = w
        else:
            # –ï—Å–ª–∏ —Å–ª–æ–≤–æ —É–∂–µ –µ—Å—Ç—å, –ø—Ä–æ–≤–µ—Ä—è–µ–º —É—Ä–æ–≤–µ–Ω—å
            existing_level = unique_words[base_word]["level"]
            if level_priority[level] < level_priority[existing_level]:
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å–ª–æ–≤–æ —Å –±–æ–ª–µ–µ –Ω–∏–∑–∫–∏–º —É—Ä–æ–≤–Ω–µ–º (A1 –ª—É—á—à–µ —á–µ–º B2)
                unique_words[base_word] = w

    return list(unique_words.values())


def sort_words_by_level(words):
    """
    –°–æ—Ä—Ç–∏—Ä—É–µ—Ç —Å–ª–æ–≤–∞: —Å–Ω–∞—á–∞–ª–∞ –ø–æ —É—Ä–æ–≤–Ω—é (A1‚ÜíC1), –ø–æ—Ç–æ–º –ø–æ –∞–ª—Ñ–∞–≤–∏—Ç—É
    """
    level_order = {level: i for i, level in enumerate(LEVEL_ORDER)}

    # –§—É–Ω–∫—Ü–∏—è –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è: —Å–Ω–∞—á–∞–ª–∞ –ø–æ —É—Ä–æ–≤–Ω—é, –ø–æ—Ç–æ–º –ø–æ –∞–Ω–≥–ª–∏–π—Å–∫–æ–º—É —Å–ª–æ–≤—É
    def sort_key(word):
        # –î–ª—è —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∏ –ø–æ –∞–ª—Ñ–∞–≤–∏—Ç—É –±–µ—Ä–µ–º –ø–µ—Ä–≤–æ–µ —Å–ª–æ–≤–æ —Å—Ç—Ä–æ–∫–∏
        base_word = re.match(r'^([a-zA-Z\-]+)', word["en"])
        sort_word = base_word.group(1).lower() if base_word else word["en"].lower()
        return (level_order[word["level"]], sort_word)

    return sorted(words, key=sort_key)


def print_statistics(words):
    """
    –í—ã–≤–æ–¥–∏—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ —É—Ä–æ–≤–Ω—è–º
    """
    stats = defaultdict(int)
    for w in words:
        stats[w["level"]] += 1

    print("\nüìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —É—Ä–æ–≤–Ω—è–º:")
    print("-" * 30)
    for level in LEVEL_ORDER:
        if level in stats:
            print(f"  {level}: {stats[level]:4d} —Å–ª–æ–≤")
    print("-" * 30)
    print(f"  –í—Å–µ–≥–æ: {len(words):4d} —Å–ª–æ–≤")

    # –ü—Ä–∏–º–µ—Ä—ã —Å–ª–æ–≤ –∫–∞–∂–¥–æ–≥–æ —É—Ä–æ–≤–Ω—è
    print("\nüî§ –ü—Ä–∏–º–µ—Ä—ã —Å–ª–æ–≤ –ø–æ —É—Ä–æ–≤–Ω—è–º (–ø–µ—Ä–≤—ã–µ 3):")
    for level in LEVEL_ORDER:
        level_words = [w["en"] for w in words if w["level"] == level][:3]
        if level_words:
            print(f"  {level}: {', '.join(level_words)}")


def main():
    all_words = []

    print("üìö –ü–∞—Ä—Å–∏–Ω–≥ Oxford —Å–ª–æ–≤–∞—Ä–µ–π –∏–∑ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö —Ñ–∞–π–ª–æ–≤")
    print("=" * 50)

    # –ü–∞—Ä—Å–∏–º –≤—Å–µ —Ñ–∞–π–ª—ã
    for level_name, filepath in INPUT_FILES.items():
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —É—Ä–æ–≤–µ–Ω—å CEFR –∏–∑ –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞
        if level_name in ["A1", "A2", "B1", "B2", "C1"]:
            level = level_name
        elif level_name == "B2_5000":
            level = "B2"  # Oxford 5000 B2 —Å–ª–æ–≤–∞
        else:
            level = level_name

        print(f"üìÑ –ß–∏—Ç–∞—é {filepath} ...")
        words = parse_level_file(Path(filepath), level)
        all_words.extend(words)
        print(f"  –ò–∑–≤–ª–µ—á–µ–Ω–æ: {len(words)} —Å–ª–æ–≤")

    print(f"\nüîÅ –£–¥–∞–ª—è—é –¥—É–±–ª–∏–∫–∞—Ç—ã (—Å–æ—Ö—Ä–∞–Ω—è—é –±–æ–ª–µ–µ –Ω–∏–∑–∫–∏–π —É—Ä–æ–≤–µ–Ω—å)...")
    before_count = len(all_words)
    all_words = deduplicate_preserve_levels(all_words)
    after_count = len(all_words)
    print(f"  –ë—ã–ª–æ: {before_count}, —Å—Ç–∞–ª–æ: {after_count} (—É–¥–∞–ª–µ–Ω–æ: {before_count - after_count})")

    print(f"\nüîÄ –°–æ—Ä—Ç–∏—Ä—É—é –ø–æ —É—Ä–æ–≤–Ω—è–º –∏ –∞–ª—Ñ–∞–≤–∏—Ç—É...")
    all_words = sort_words_by_level(all_words)

    # –î–æ–±–∞–≤–ª—è–µ–º ID
    for i, word in enumerate(all_words, 1):
        word["id"] = i

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
    with open(OUTPUT_FILE, "w", encoding="utf-8") as f:
        json.dump(all_words, f, ensure_ascii=False, indent=2)

    print(f"\n‚úÖ –ì–æ—Ç–æ–≤–æ!")
    print(f"üíæ –§–∞–π–ª: {OUTPUT_FILE}")

    # –í—ã–≤–æ–¥–∏–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
    print_statistics(all_words)

    # –ü—Ä–∏–º–µ—Ä –ø–µ—Ä–≤—ã—Ö 20 —Å–ª–æ–≤
    print(f"\nüìù –ü–µ—Ä–≤—ã–µ 20 —Å–ª–æ–≤:")
    print("-" * 50)
    for i, word in enumerate(all_words[:20], 1):
        print(f"{word['id']:4d}. {word['en']:30s} [{word['level']}]")

    # –ü—Ä–∏–º–µ—Ä –ø–æ—Å–ª–µ–¥–Ω–∏—Ö 5 —Å–ª–æ–≤
    print(f"\nüìù –ü–æ—Å–ª–µ–¥–Ω–∏–µ 5 —Å–ª–æ–≤:")
    print("-" * 50)
    for word in all_words[-5:]:
        print(f"{word['id']:4d}. {word['en']:30s} [{word['level']}]")


if __name__ == "__main__":
    main()